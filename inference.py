import os
import cv2
import torch
import torch.nn as nn

import pandas as pd
import torch.nn.functional as F
from torchvision import models

from utils.config import IMAGE_SIZE, LABELS, THRESHOLD, BATCH_SIZE, NUM_CLASSES
from datasets.dataloader import get_test_data_loaders

from models.teacher_eff import CustomEfficientNet
import wandb
import time

thres = THRESHOLD

def load_single_model(model_path, model_type="student", num_classes=6):
    """
    ë³‘í•©ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    if model_type == "teacher":
        model = models.efficientnet_b0(weights=None)
        model.classifier[1] = nn.Linear(1280, num_classes)
    elif model_type == "student":
        model = models.mobilenet_v3_small(weights=None)
        model.classifier[3] = nn.Linear(1024, num_classes)
        # model = models.shufflenet_v2_x1_0(weights=None)
        # model.fc = nn.Linear(1024, num_classes)
    else:
        raise ValueError("model_typeì€ 'teacher' ë˜ëŠ” 'student'ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    model.load_state_dict(torch.load(model_path, map_location="cuda" if torch.cuda.is_available() else "cpu"))
    model.eval().cuda() if torch.cuda.is_available() else model.eval()
    return model


def infer_single_model(test_loader, model, class_labels, csv_file_path, model_type, device="cuda"):
    """
    ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ DataLoaderì—ì„œ Inference ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥.
    - âœ… GPU ì—°ì‚° ë™ê¸°í™”í•˜ì—¬ ì •í™•í•œ ì¶”ë¡  ì†ë„ ì¸¡ì •
    - âœ… WandB Tableì„ í™œìš©í•˜ì—¬ Inference ê²°ê³¼ ë¡œê¹…
    - âœ… CSV íŒŒì¼ì— ì˜ˆì¸¡ ê²°ê³¼ì™€ Latency ì €ì¥

    Args:
        test_loader (DataLoader): ì¶”ë¡ í•  ë°ì´í„° ë¡œë”
        model (torch.nn.Module): ë¡œë“œëœ ë‹¨ì¼ ëª¨ë¸
        class_labels (list): í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        csv_file_path (str): ê²°ê³¼ë¥¼ ì €ì¥í•  CSV ê²½ë¡œ
        model_type (str): "teacher" ë˜ëŠ” "student" (WandB ë¡œê¹… êµ¬ë¶„ìš©)
        device (str): "cuda" ë˜ëŠ” "cpu"
    """
    model.to(device)
    model.eval()
    results = []

    # âœ… WandB ì„¸ì…˜ ì‹œì‘
    wandb.init(project="document_classification", name=f"Inference_{model_type}", group="Inference")
    table = wandb.Table(columns=["íŒŒì¼ëª…", "ì˜ˆì¸¡ ë ˆì´ë¸”", "í™•ë¥ ", "ì¶”ë¡  ì‹œê°„(ms)"])

    # âœ… CSV íŒŒì¼ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    if os.path.exists(csv_file_path):
        df = pd.read_csv(csv_file_path)
    else:
        print(f"âš ï¸ Warning: {csv_file_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        df = pd.DataFrame(columns=["íŒŒì¼ëª…", "ì˜ˆì¸¡ ë ˆì´ë¸”", "í™•ë¥ ", "ì¶”ë¡  ì‹œê°„(ms)"])

    # âœ… ì‹¤ì‹œê°„ ì¶”ë¡  ì†ë„ ì¸¡ì • (Batch ê¸°ë°˜ Latency ì¸¡ì •)
    x = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(device)
    with torch.no_grad():
        for _ in range(10):  # âœ… Warm-up (GPU ìµœì í™”)
            _ = model(x)

        start_time = torch.cuda.Event(enable_timing=True)
        end_time = torch.cuda.Event(enable_timing=True)

        start_time.record()
        _ = model(x)
        end_time.record()

        torch.cuda.synchronize()  # âœ… GPU ì—°ì‚° ë™ê¸°í™”
        latency = start_time.elapsed_time(end_time)  # ms ë‹¨ìœ„
        print(f"\nğŸš€ ì‹¤ì‹œê°„ ì¶”ë¡  ì†ë„: {latency:.3f} ms (1 ì´ë¯¸ì§€ë‹¹)\n")

    # âœ… Inference ìˆ˜í–‰
    total_latency = 0
    total_images = 0

    with torch.no_grad():
        for images, file_names in test_loader:
            images = images.to(device)

            batch_start_time = torch.cuda.Event(enable_timing=True)
            batch_end_time = torch.cuda.Event(enable_timing=True)

            batch_start_time.record()
            outputs = model(images)
            batch_end_time.record()

            torch.cuda.synchronize()  # âœ… GPU ì—°ì‚° ë™ê¸°í™”
            batch_latency = batch_start_time.elapsed_time(batch_end_time)  # ms ë‹¨ìœ„

            # âœ… í‰ê·  Latency ê³„ì‚°ì„ ìœ„í•´ ëˆ„ì 
            total_latency += batch_latency
            total_images += len(file_names)

            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()
            confidences = probabilities.max(dim=1)[0].cpu().numpy()

            for file_name, pred_class, confidence in zip(file_names, predicted_classes, confidences):
                predicted_label = class_labels[pred_class]

                # âœ… CSV íŒŒì¼ ì—…ë°ì´íŠ¸
                if file_name in df["íŒŒì¼ëª…"].values:
                    df.loc[df["íŒŒì¼ëª…"] == file_name, ["ì˜ˆì¸¡ ë ˆì´ë¸”", "í™•ë¥ ", "ì¶”ë¡  ì‹œê°„(ms)"]] = [predicted_label, confidence, batch_latency / len(file_names)]
                else:
                    df = df.append({"íŒŒì¼ëª…": file_name, "ì˜ˆì¸¡ ë ˆì´ë¸”": predicted_label, "í™•ë¥ ": confidence, "ì¶”ë¡  ì‹œê°„(ms)": batch_latency / len(file_names)}, ignore_index=True)

                # âœ… WandB Tableì— ì €ì¥
                table.add_data(file_name, predicted_label, confidence, batch_latency / len(file_names))

                results.append((file_name, predicted_label, confidence, batch_latency / len(file_names)))


    df.to_csv(csv_file_path, index=False, encoding="utf-8-sig")
    wandb.log({"Inference Results": table, "Average Latency (ms)": latency})
    

    answer = ["ì²´ë ¥í‰ê°€", "ìƒí™œê¸°ë¡ë¶€ëŒ€ì²´ì–‘ì‹", "ì£¼ë¯¼ë“±ë¡ë³¸", "êµ­ë¯¼ì²´ë ¥100", "ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ìì¦ëª…ì„œ",
              "êµ­ë¯¼ì²´ë ¥100", "ê²€ì •ê³ ì‹œí•©ê²©ìì¦ëª…ì„œ", "ì²´ë ¥í‰ê°€", "ìƒí™œê¸°ë¡ë¶€ëŒ€ì²´ì–‘ì‹", "ê²€ì •ê³ ì‹œí•©ê²©ìì¦ëª…ì„œ",
              "ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ìì¦ëª…ì„œ", "ì£¼ë¯¼ë“±ë¡ë³¸", "ê²€ì •ê³ ì‹œí•©ê²©ìì¦ëª…ì„œ"]
    idx = 0

    for filename, pred_class, conf, latency in results:
        print(f"{filename}: {answer[idx]} â†’ {pred_class} (Confidence: {conf:.4f}, Latency: {latency:.3f} ms)")
        idx += 1

    wandb.finish()



if __name__ == "__main__":

    type_list = ["teacher", "student"]
    model_type = type_list[1] # 0: "teacher", 1: "student"
    print(f"\n===== Inference Results(model type: {model_type}) =====")

    class_labels = LABELS
    batch_size = 1
    num_classes = NUM_CLASSES

    test_folder_path = "/data/ephemeral/home/aims_image_classifier/data"
    model_path = f"/data/ephemeral/home/aims_image_classifier/model_pth/{model_type}_model.pth"
    csv_file_path = f"/data/ephemeral/home/aims_image_classifier/data/inference_labels_{model_type}.csv"
    
    model = load_single_model(model_path, model_type=model_type, num_classes=num_classes)
    test_loader = get_test_data_loaders(test_folder_path, batch_size)

    infer_single_model(test_loader, model, class_labels, csv_file_path, model_type)
    
