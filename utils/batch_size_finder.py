import torch
from torchvision import models

def find_max_batch_size(model, image_size=(3, 512, 512), max_mem_ratio=0.9):
    """
    GPU ë©”ëª¨ë¦¬ë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ” ìµœëŒ€ batch_sizeë¥¼ íƒìƒ‰í•˜ëŠ” í•¨ìˆ˜
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    batch_size = 1
    max_batch_size = 1

    while True:
        try:
            dummy_input = torch.randn(batch_size, *image_size).to(device)
            _ = model(dummy_input)  # ì¶”ë¡  í…ŒìŠ¤íŠ¸
            torch.cuda.synchronize()

            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            allocated_mem = torch.cuda.memory_allocated(device)
            total_mem = torch.cuda.get_device_properties(device).total_memory

            # ìµœëŒ€ ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì‚¬ìš©ë¥  ê³„ì‚°
            if allocated_mem / total_mem > max_mem_ratio:
                break  # ë©”ëª¨ë¦¬ ì´ˆê³¼ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´ ì¤‘ë‹¨

            max_batch_size = batch_size
            batch_size += 1  # ë°°ì¹˜ í¬ê¸° ì¦ê°€

        except RuntimeError:  # OOM ë°œìƒí•˜ë©´ ì¤‘ë‹¨
            break

    return max_batch_size


# model = models.mobilenet_v3_small(weights=None)
# model.classifier[3] = torch.nn.Linear(1024, 6)

model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
model.classifier[1] = torch.nn.Linear(1280, 6)


optimal_batch_size = find_max_batch_size(model)
print(f"ğŸš€ ìµœì ì˜ batch_size: {optimal_batch_size}")
