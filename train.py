import os
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont

import torch
import torch.nn as nn
import torch.optim as optim
import torch.amp as amp  # Mixed Precision Training
import gc
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from utils.eda.set_font_matplot import set_nanumgothic_font
set_nanumgothic_font()

from loss import DistillationLoss, FeatureDistillationLoss
from utils.early_stopping import EarlyStopping
from utils.config import LABELS, EPOCHES, LEARNING_RATE, BATCH_SIZE

import wandb


def train_teacher(teacher, train_loader, val_loader, epochs, lr, num_classes, fold_idx, patience):
    #teacher.base_model.classifier[1] = nn.Linear(1280, num_classes)
    teacher.classifier[1] = nn.Linear(1280, num_classes)
    teacher = teacher.train().cuda()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(teacher.parameters(), lr=lr)
    scaler = amp.GradScaler()
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=2)
    early_stopping = EarlyStopping(patience=patience)

    best_val_loss = float("inf")  # Best Validation Loss ì´ˆê¸°í™”

    for epoch in range(epochs):
        torch.cuda.empty_cache()
        gc.collect()
        teacher.train()
        total_loss = 0

        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            with amp.autocast("cuda"):
                outputs = teacher(images)
                loss = criterion(outputs, labels)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss = validate(teacher, val_loader, LABELS)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}")

        scheduler.step(val_loss)
        early_stopping(val_loss)

        model_dir = "model_pth"
        os.makedirs(model_dir, exist_ok=True)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = os.path.join(model_dir, f"teacher_model_fold{fold_idx}.pth")
            torch.save(teacher.state_dict(), model_save_path)
            print(f"Best Model Updated: {model_save_path} (Validation Loss: {best_val_loss:.4f})")

        if early_stopping.early_stop:
            print("Early Stopping ì ìš©! í•™ìŠµ ì¤‘ë‹¨")
            break
    


# ëª¨ë¸ ê²€ì¦ í•¨ìˆ˜
def validate(model, val_loader, class_labels, save_dir="validation_visualization"):
    """
    ëª¨ë¸ ê²€ì¦ í•¨ìˆ˜ (ì •í™•ë„, ì†ì‹¤ ê³„ì‚° + ì˜ˆì¸¡ ê²°ê³¼ ì´ë¯¸ì§€ì— í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ í›„ ì €ì¥)
    
    Args:
        model (torch.nn.Module): ê²€ì¦í•  ëª¨ë¸
        val_loader (DataLoader): ê²€ì¦ ë°ì´í„° ë¡œë”
        class_labels (list): í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        save_dir (str): ì‹œê°í™”ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ
    """
    # âœ… ì €ì¥í•  í´ë” ìƒì„±
    os.makedirs(save_dir, exist_ok=True)

    # âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”)
    font_path = "/data/ephemeral/home/aims_image_classifier/utils/eda/fonts/NanumGothicCoding.ttf"
    font = ImageFont.truetype(font_path, 30)

    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for batch_idx, (images, labels) in enumerate(val_loader):
            images, labels = images.cuda(), labels.cuda()
            outputs = model(images)
            loss = criterion(outputs, labels)

            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            total_loss += loss.item()

            for idx in range(images.shape[0]):
                true_label = labels[idx].cpu().item()
                pred_label = preds[idx].cpu().item()
                true_class = class_labels[true_label]
                pred_class = class_labels[pred_label]

                image_np = images[idx].cpu().numpy().transpose(1, 2, 0)  # (C, H, W) â†’ (H, W, C)
                image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # ì •ê·œí™” í•´ì œ

                image_pil = Image.fromarray(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))
                draw = ImageDraw.Draw(image_pil)

                color = (0, 255, 0) if true_class == pred_class else (255, 0, 0)  # ë§ìœ¼ë©´ ì´ˆë¡, í‹€ë¦¬ë©´ ë¹¨ê°•
                draw.text((10, 30), f"ì •ë‹µ: {true_class} | ì˜ˆì¸¡: {pred_class}", font=font, fill=color)

                annotated_image = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)

                save_path = os.path.join(save_dir, f"val_{batch_idx}_{idx}.png")
                cv2.imwrite(save_path, annotated_image)

    accuracy = 100 * correct / total
    avg_loss = total_loss / len(val_loader)
    
    print(f"\nValidation Accuracy: {accuracy:.2f}%, Validation Loss: {avg_loss:.4f}")

    return avg_loss


# MobileNet Student ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ì§€ì‹ ì¦ë¥˜ ì ìš©)
def train_student(teacher, student, train_loader, val_loader, epochs, lr, temperature, alpha, num_classes, fold_idx, patience):
    """
    K-Fold êµì°¨ ê²€ì¦ì„ ì ìš©í•œ Student ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜.

    Args:
        teacher (nn.Module): Teacher ëª¨ë¸
        student (nn.Module): MobileNet Student ëª¨ë¸
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        epochs (int): í•™ìŠµí•  Epoch ìˆ˜
        lr (float): Learning Rate
        temperature (float): ì§€ì‹ ì¦ë¥˜ ì˜¨ë„ ê°’
        alpha (float): ì§€ì‹ ì¦ë¥˜ ê°€ì¤‘ì¹˜
        num_classes (int): í´ë˜ìŠ¤ ê°œìˆ˜
        fold_idx (int): í˜„ì¬ K-Foldì˜ ì¸ë±ìŠ¤ (Fold ë²ˆí˜¸)
        n_splits (int): K-Fold ê°œìˆ˜
        random_seed (int): ëœë¤ ì‹œë“œ ê°’
    """
    # âœ… ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    model_dir = "model_pth"
    os.makedirs(model_dir, exist_ok=True)

    # teacher_model_path = os.path.join(model_dir, f"teacher_model_fold{fold_idx}.pth")
    # teacher.load_state_dict(torch.load(teacher_model_path))
    teacher = teacher.eval().cuda()

    student.classifier[3] = nn.Linear(1024, num_classes)  # MobileNet
    student = student.train().cuda()

    criterion = DistillationLoss(temperature, alpha)
    optimizer = optim.Adam(student.parameters(), lr=lr)
    scaler = amp.GradScaler()
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=2)
    early_stopping = EarlyStopping(patience=patience)

    best_val_loss = float("inf")  # Best Validation Loss ì´ˆê¸°í™”

    for epoch in range(epochs):
        torch.cuda.empty_cache()
        gc.collect()
        student.train()
        total_loss = 0

        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            with torch.no_grad():
                teacher_logits = teacher(images)

            with amp.autocast("cuda"):
                student_logits = student(images)
                loss = criterion(student_logits, teacher_logits, labels)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss = validate(student, val_loader, LABELS)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}")

        scheduler.step(val_loss)
        early_stopping(val_loss)
 
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = os.path.join(model_dir, f"student_model_fold{fold_idx}.pth")
            torch.save(student.state_dict(), model_save_path)
            print(f"Best Model Updated: {model_save_path} (Validation Loss: {best_val_loss:.4f})")

        if early_stopping.early_stop:
            print("Early Stopping ì ìš©! í•™ìŠµ ì¤‘ë‹¨")
            break


# Feature Distillationìš© MSE Loss
criterion_feature_distill = nn.MSELoss()


def get_intermediate_features(model, layer_name):
    """
    íŠ¹ì • ë ˆì´ì–´ì˜ Feature Mapì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ Hook ë“±ë¡ í•¨ìˆ˜
    """
    features = {}

    def hook(module, input, output):
        features[layer_name] = output

    layer = dict([*model.named_modules()])[layer_name]  # ì›í•˜ëŠ” ë ˆì´ì–´ ì„ íƒ
    layer.register_forward_hook(hook)
    
    return features


def train_student_with_fd(teacher, student, train_loader, val_loader, epochs, lr, temperature, alpha, num_classes, fold_idx, patience):
    model_dir = "model_pth"
    os.makedirs(model_dir, exist_ok=True)

    teacher_model_path = os.path.join(model_dir, f"teacher_model_fold{fold_idx}.pth")
    teacher.load_state_dict(torch.load(teacher_model_path))
    teacher = teacher.eval().cuda()

    student.classifier[3] = nn.Linear(1024, num_classes)  # MobileNet
    student = student.train().cuda()

    # âœ… Feature Distillation Loss ì¶”ê°€
    criterion_feature = FeatureDistillationLoss(alpha=0.7, temperature=4.0, feature_weight=0.2)

    optimizer = optim.Adam(student.parameters(), lr=lr)
    scaler = amp.GradScaler()
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=2)
    early_stopping = EarlyStopping(patience=patience)

    best_val_loss = float("inf")

    for epoch in range(epochs):
        torch.cuda.empty_cache()
        gc.collect()
        student.train()
        total_loss = 0

        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            with torch.no_grad():
                teacher_logits, teacher_features_out = teacher(images)

            with amp.autocast("cuda"):
                student_logits, student_features_out = student(images)

                # âœ… Feature Distillation Loss ê³„ì‚°
                loss_fd = criterion_feature(student_logits, teacher_logits, student_features_out, teacher_features_out, labels)

            optimizer.zero_grad()
            scaler.scale(loss_fd).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss_fd.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss = validate(student, val_loader, LABELS)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}")

        scheduler.step(val_loss)
        early_stopping(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = os.path.join(model_dir, f"student_model_fold{fold_idx}.pth")
            torch.save(student.state_dict(), model_save_path)
            print(f"âœ… Best Model Updated: {model_save_path} (Validation Loss: {best_val_loss:.4f})")

        if early_stopping.early_stop:
            print("Early Stopping ì ìš©! í•™ìŠµ ì¤‘ë‹¨")
            break


def validate_va(model, val_loader, class_labels):
    """
    ëª¨ë¸ ê²€ì¦ í•¨ìˆ˜
    """
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.cuda(), labels.cuda()
            outputs = model(images)
            loss = criterion(outputs, labels)

            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            total_loss += loss.item()

    accuracy = 100 * correct / total
    avg_loss = total_loss / len(val_loader) 

    print(f"Validation Accuracy: {accuracy:.2f}%, Validation Loss: {avg_loss:.4f}")

    return avg_loss, accuracy 


def train_student_with_va(teacher, student, train_loader, val_loader, epochs, lr, temperature, alpha, num_classes, fold_idx, patience):
    """
    K-Fold êµì°¨ ê²€ì¦ì„ ì ìš©í•œ Student ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜.

    Args:
        teacher (nn.Module): Teacher ëª¨ë¸
        student (nn.Module): MobileNet Student ëª¨ë¸
        train_loader (DataLoader): í•™ìŠµ ë°ì´í„° ë¡œë”
        val_loader (DataLoader): ê²€ì¦ ë°ì´í„° ë¡œë”
        epochs (int): í•™ìŠµí•  Epoch ìˆ˜
        lr (float): Learning Rate
        temperature (float): ì§€ì‹ ì¦ë¥˜ ì˜¨ë„ ê°’
        alpha (float): ì§€ì‹ ì¦ë¥˜ ê°€ì¤‘ì¹˜
        num_classes (int): í´ë˜ìŠ¤ ê°œìˆ˜
        fold_idx (int): í˜„ì¬ K-Foldì˜ ì¸ë±ìŠ¤ (Fold ë²ˆí˜¸)
        patience (int): Early Stopping patience ê°’
    """

    model_dir = "model_pth"
    os.makedirs(model_dir, exist_ok=True)

    val_acc_file = os.path.join(model_dir, f"validation_accuracies.txt")

    teacher = teacher.eval().cuda()

    student.classifier[3] = torch.nn.Linear(1024, num_classes)
    student = student.train().cuda()

    criterion = DistillationLoss(temperature, alpha)
    optimizer = optim.Adam(student.parameters(), lr=lr)
    scaler = amp.GradScaler()
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=2)
    early_stopping = EarlyStopping(patience=patience)

    best_val_loss = float("inf")
    best_val_acc = 0.0

    for epoch in range(epochs):
        torch.cuda.empty_cache()
        gc.collect()
        student.train()
        total_loss = 0

        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            with torch.no_grad():
                teacher_logits = teacher(images)

            with amp.autocast("cuda"):
                student_logits = student(images)
                loss = criterion(student_logits, teacher_logits, labels)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss, val_acc = validate_va(student, val_loader, LABELS)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}%")

        scheduler.step(val_loss)
        early_stopping(val_loss)

        with open(val_acc_file, "a") as f:
            f.write(f"Fold {fold_idx}, Epoch {epoch+1}, Validation Accuracy: {val_acc:.2f}%\n")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_val_acc = val_acc
            model_save_path = os.path.join(model_dir, f"student_model_fold{fold_idx}.pth")
            torch.save(student.state_dict(), model_save_path)
            print(f"âœ… Best Model Updated: {model_save_path} (Validation Loss: {best_val_loss:.4f}, Accuracy: {best_val_acc:.2f}%)")

        if early_stopping.early_stop:
            print("âœ… Early Stopping ì ìš©! í•™ìŠµ ì¤‘ë‹¨")
            break


def train_teacher_all(teacher, train_loader, epochs, lr, num_classes, patience):
    """
    Teacher ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜ (Validation ì—†ì´ Trainë§Œ ìˆ˜í–‰)

    Args:
        teacher (nn.Module): Teacher ëª¨ë¸
        epochs (int): í•™ìŠµí•  Epoch ìˆ˜
        lr (float): Learning Rate
        num_classes (int): í´ë˜ìŠ¤ ê°œìˆ˜
    """

    teacher.classifier[1] = nn.Linear(1280, num_classes)  # EfficientNet
    teacher = teacher.train().cuda()

    criterion = nn.CrossEntropyLoss()

    optimizer = optim.AdamW(teacher.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)
    scaler = amp.GradScaler()

    model_dir = "model_pth"
    os.makedirs(model_dir, exist_ok=True)

    best_train_loss = float("inf")  # Best Train Loss ì´ˆê¸°í™”

    wandb.init(
    project="document_classification",
    name="EfficientNet-B0",
    group="Teacher",
    config={
        "epochs": EPOCHES,
        "batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE,
        "optimizer": optimizer.__class__.__name__,
        "scheduler": scheduler.__class__.__name__
    }
)
    patience_counter = 0
    early_stopping_patience = patience

    for epoch in range(epochs):
        torch.cuda.empty_cache()
        gc.collect()
        teacher.train()
        total_loss = 0

        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            with amp.autocast("cuda"):
                outputs = teacher(images)
                loss = criterion(outputs, labels)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}")

        scheduler.step(avg_train_loss)
        #scheduler.step()
        
        wandb.log({"Techer Train Loss": avg_train_loss, "Techer Learning Rate": optimizer.param_groups[0]['lr'], "Techer Epoch": epoch + 1})

        if avg_train_loss < best_train_loss:
            best_train_loss = avg_train_loss
            model_save_path = os.path.join(model_dir, f"teacher_model.pth")
            torch.save(teacher.state_dict(), model_save_path)
            print(f"Best Model Updated: {model_save_path} (Train Loss: {best_train_loss:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= early_stopping_patience:
            print("Early Stopping")
            break
    
    wandb.finish()


# def train_student_all(teacher, student, train_loader, epochs, lr, temperature, alpha, num_classes, patience):
#     """
#     Teacher ëª¨ë¸ì„ í™œìš©í•˜ì—¬ Student ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜ (Validation ì—†ì´ Trainë§Œ ìˆ˜í–‰)

#     Args:
#         teacher (nn.Module): Teacher ëª¨ë¸
#         student (nn.Module): MobileNet Student ëª¨ë¸
#         train_loader (DataLoader): Train ë°ì´í„° ë¡œë”
#         epochs (int): í•™ìŠµí•  Epoch ìˆ˜
#         lr (float): Learning Rate
#         temperature (float): ì§€ì‹ ì¦ë¥˜ ì˜¨ë„ ê°’
#         alpha (float): ì§€ì‹ ì¦ë¥˜ ê°€ì¤‘ì¹˜
#         num_classes (int): í´ë˜ìŠ¤ ê°œìˆ˜
#     """

#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     model_dir = "model_pth"
#     os.makedirs(model_dir, exist_ok=True)

#     teacher = teacher.to(device).eval()

#     student.classifier[3] = nn.Linear(1024, num_classes)  # MobileNet
#     student = student.to(device).train()

#     from utils.pruning import apply_pruning
#     student = apply_pruning(student, amount=0.3)

#     criterion = DistillationLoss(temperature, alpha)
#     optimizer = optim.Adam(student.parameters(), lr=lr)
#     scaler = amp.GradScaler()
#     scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=2)

#     student = torch.quantization.quantize_dynamic(student, {nn.Linear}, dtype=torch.qint8)

#     # Early Stopping ë³€ìˆ˜
#     best_train_loss = float("inf")
#     patience_counter = 0
#     early_stopping_patience = patience

#     for epoch in range(epochs):
#         torch.cuda.empty_cache()
#         gc.collect()
#         student.train()
#         total_loss = 0

#         for images, labels in train_loader:
#             images, labels = images.to(device), labels.to(device)

#             with torch.no_grad():
#                 teacher_logits = teacher(images)

#             with amp.autocast(device_type="cuda"):
#                 student_logits = student(images)
#                 loss = criterion(student_logits, teacher_logits, labels)

            
#             optimizer.zero_grad()
#             scaler.scale(loss).backward()
#             scaler.step(optimizer)
#             scaler.update()

#             total_loss += loss.item()

#         avg_train_loss = total_loss / len(train_loader)
#         scheduler.step(avg_train_loss)

#         print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}")

#         # Train Lossê°€ ìµœì†Œì¼ ë•Œ ëª¨ë¸ ì €ì¥
#         if avg_train_loss < best_train_loss:
#             best_train_loss = avg_train_loss
#             model_save_path = os.path.join(model_dir, "student_model.pth")
#             torch.save(student.state_dict(), model_save_path)
#             print(f"âœ… Best Model Updated: {model_save_path} (Train Loss: {best_train_loss:.4f})")
#             patience_counter = 0
#         else:
#             patience_counter += 1

#         # Early Stopping ì ìš©
#         if patience_counter >= early_stopping_patience:
#             print("Early Stopping")
#             break

import torch.nn.utils.prune as prune
def train_student_all(teacher, student, train_loader, epochs, lr, temperature, alpha, num_classes, patience):
    """
    Teacher ëª¨ë¸ì„ í™œìš©í•˜ì—¬ Student ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜ (Validation ì—†ì´ Trainë§Œ ìˆ˜í–‰)

    Args:
        teacher (nn.Module): Teacher ëª¨ë¸
        student (nn.Module): MobileNet Student ëª¨ë¸
        train_loader (DataLoader): Train ë°ì´í„° ë¡œë”
        epochs (int): í•™ìŠµí•  Epoch ìˆ˜
        lr (float): Learning Rate
        temperature (float): ì§€ì‹ ì¦ë¥˜ ì˜¨ë„ ê°’
        alpha (float): ì§€ì‹ ì¦ë¥˜ ê°€ì¤‘ì¹˜
        num_classes (int): í´ë˜ìŠ¤ ê°œìˆ˜
        patience (int): Early Stopping ê¸°ì¤€
    """
    import matplotlib.pyplot as plt

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model_dir = "model_pth"
    feature_map_dir = "feature_maps"
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(feature_map_dir, exist_ok=True)

    teacher = teacher.to(device).eval()

    # âœ… MobileNet Studentì˜ ì¶œë ¥ì¸µ ì¡°ì •
    student.classifier[3] = nn.Linear(1024, num_classes)  # MobileNet
    # student.fc = nn.Linear(1024, num_classes)  # ShuffleNetV2
    student = student.to(device).train()

    # from torch.nn.utils import prune

    # prune.ln_structured(student.features[0][0], name="weight", amount=0.5, n=2, dim=0) 
    # if hasattr(student.features[0][0], "weight_orig") and hasattr(student.features[0][0], "weight_mask"):
    #     prune.remove(student.features[0][0], "weight")
    #     print("âœ… Pruning removed successfully!")
    # else:
    #     print("âš ï¸ Pruning was not applied properly, cannot remove.")
    # print("Pruned parameters:", list(student.features[0][0]._parameters.keys()))
    # print("Buffers:", list(student.features[0][0]._buffers.keys()))

    criterion = DistillationLoss(temperature, alpha)
    
    optimizer = optim.AdamW(student.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)

    scaler = amp.GradScaler()

    wandb.init(
    project="document_classification",
    name="MobileNetV3-Small",
    group="Student",
    config={
        "epochs": EPOCHES,
        "batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE,
        "optimizer": optimizer.__class__.__name__,
        "scheduler": scheduler.__class__.__name__
    }
)

    # âœ… Hookì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ë§µ ì €ì¥
    activation = {}

    def get_activation(name):
        def hook(model, input, output):
            activation[name] = output.detach().cpu().numpy()
        return hook

    # âœ… íŠ¹ì • ë ˆì´ì–´ ì„ íƒ (MobileNetV3 ê¸°ì¤€: ì²« ë²ˆì§¸ Conv ë ˆì´ì–´)
    hook_layer_name = "features.0"  # ì²« ë²ˆì§¸ Conv ë ˆì´ì–´
    hook_handle = getattr(student.features, "0").register_forward_hook(get_activation(hook_layer_name))

    # âœ… Early Stopping ë³€ìˆ˜
    best_train_loss = float("inf")
    patience_counter = 0
    early_stopping_patience = patience

    for epoch in range(epochs):
        torch.cuda.empty_cache()
        gc.collect()
        student.train()
        total_loss = 0

        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)

            with torch.no_grad():
                teacher_logits = teacher(images)

            with amp.autocast(device_type="cuda"):
                student_logits = student(images)
                loss = criterion(student_logits, teacher_logits, labels)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

            # âœ… íŠ¹ì • ë°°ì¹˜ì—ì„œ íŠ¹ì§•ë§µ ì €ì¥ (1ê°œ ìƒ˜í”Œë§Œ ì €ì¥)
            if batch_idx == 0:  
                feature_map = activation[hook_layer_name][0]  # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ íŠ¹ì§•ë§µ ì €ì¥
                num_features = feature_map.shape[0]  # íŠ¹ì§•ë§µ ê°œìˆ˜

                # âœ… íŠ¹ì§•ë§µ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
                save_path = os.path.join(feature_map_dir, f"epoch_{epoch+1}.npy")
                np.save(save_path, feature_map)
                print(f"ğŸ“Œ Feature Map saved at: {save_path}")

                # âœ… íŠ¹ì§•ë§µ ì‹œê°í™” (16ê°œë§Œ ì„ íƒ)
                fig, axes = plt.subplots(4, 4, figsize=(20, 20))
                for i, ax in enumerate(axes.flat):
                    if i < num_features:
                        ax.imshow(feature_map[i], cmap="viridis")
                        ax.axis("off")
                plt.suptitle(f"Epoch {epoch+1} Feature Maps")
                plt.savefig(os.path.join(feature_map_dir, f"epoch_{epoch+1}.png"))
                plt.close()

        avg_train_loss = total_loss / len(train_loader)
        scheduler.step(avg_train_loss)
        #scheduler.step()
        
        wandb.log({"Student Train Loss": avg_train_loss, "Student Learning Rate": optimizer.param_groups[0]['lr'], "Student Epoch": epoch + 1})

        print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}")

        # âœ… Train Lossê°€ ìµœì†Œì¼ ë•Œ ëª¨ë¸ ì €ì¥
        if avg_train_loss < best_train_loss:
            best_train_loss = avg_train_loss
            model_save_path = os.path.join(model_dir, "student_model.pth")

            torch.save(student.state_dict(), model_save_path)
            print(f"âœ… Best Model Updated: {model_save_path} (Train Loss: {best_train_loss:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1

        # âœ… Early Stopping ì ìš©
        if patience_counter >= early_stopping_patience:
            print("Early Stopping")
            break

    # âœ… Hook ì œê±°
    hook_handle.remove()
    wandb.finish()
